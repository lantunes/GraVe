The models in this directory were created with the following settings:

<table>
 <tr>
    <th colspan="4">&nbsp;</th>
    <th colspan="8">GloVe</th>
 </tr>
 <tr>
    <th>Model</th>
    <th>Walk Method</th>
    <th>Walks</th>
    <th>Walk Length</th>
    <th>Window</th>
    <th>Components</th>
    <th>Epochs</th>
    <th>Learning Rate</th>
    <th>Workers</th>
    <th>alpha</th>
    <th>max_count</th>
    <th>max_loss</th>
 </tr>
 <tr>
    <td>karate.*</td>
    <td>DeepWalk</td>
    <td>10</td>
    <td>40</td>
    <td>10</td>
    <td>2</td>
    <td>10</td>
    <td>0.05</td>
    <td>1</td>
    <td>0.75</td>
    <td>100</td>
    <td>10.0</td>
 </tr>
 <tr>
    <td>karate2.*</td>
    <td>DeepWalk</td>
    <td>160</td>
    <td>40</td>
    <td>1</td>
    <td>2</td>
    <td>200</td>
    <td>0.05</td>
    <td>1</td>
    <td>0.75</td>
    <td>100</td>
    <td>10.0</td>
 </tr>
 <tr>
    <td>karate3.*</td>
    <td>node2vec (p=1,q=1)</td>
    <td>160</td>
    <td>40</td>
    <td>10</td>
    <td>2</td>
    <td>200</td>
    <td>0.05</td>
    <td>1</td>
    <td>0.75</td>
    <td>100</td>
    <td>10.0</td>
 </tr>
</table>

Compared to the `karate` model, the `karate2` model was trained on a larger walk dataset, as well as with many more 
epochs and a symmetric context window size of 1. The embeddings in the `karate` model appear to be clustered according 
to node degree, whereas the embeddings in the `karate2` model appear to be clustered according to community structure.
  
The `karate3` model was trained on a walk dataset generated by the node2vec approach. The embeddings in this model are
clustered according to community structure, but the GloVe context window size was 10.

The `cora` model was created with:
```
python glove_training.py --create examples/cora.node2vec.walks --components 128 --train 200 --window 10
```
The walks file for the `cora` model was created with:
```
python node2vec_walks.py --num-walks 160 --walk-length 40 --input examples/cora.adjlist --output cora --directed
```

The `karate2.fm.model` was created with the `grave.FactorizationMachine` class. It uses the `karate2.walks.0` corpus. 
The code that generated this model is in `karate2_fm_training.py`. The results are essentially the same as when using
the glove_python library, as expected, as when no extra node attributes are present, the Factorization Machine reduces 
to the GloVe approach.

The `karate2_labels.fm.model` was created with the `grave.FactorizationMachine` class, and it incorporates one of two
labels on each node as additional node features. It uses the `karate2.walks.0` corpus. The code that generated this 
model is in `karate2_labels_fm_training.py`.